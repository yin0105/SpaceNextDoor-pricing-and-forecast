---
title: "HedonicModel3:SND"
author: "luismor"
date: "3/26/2021"
output:
  pdf_document: default
  html_document: default
---

## VARIABLES SELECTION

## Loading the data

```{r hist}
library(readxl)
df <- read_excel("/Users/Unimooc/Dropbox/2021/Directorio R/SpaceNextDoor/NextDoor/Optimal pricing/NextDoor.xlsx",
                 sheet = "Data2")
```

```{r head, results='hide'}
head(df)

```

## Visualization

```{r packages visual, message= FALSE, results='hide'}
library(ggplot2)
library(corrplot)
library(tidyverse)
library(MASS)
```

```{r histogram}
hist(df$price_cents)
#The histogram shows a positive skweness (positioned to the left). There are many outliers that should be integrated in the sample as it increases over time. 
```

```{r}
gcaja <- boxplot(df$price_cents)
df<-df[!(df$price_cents %in% gcaja$out),]
hist(df$price_cents)
plot(df$price_cents,df$area)
plot(df$price_cents,df$areaPMP)

#Deleting outlyers
```

```{r correlation matrix, results='hide'}
df.cor <- cor(df[,c(8:29)], method = "kendall")
round(df.cor, digits = 1)
```

```{r corrplot}
corrplot(df.cor, method ="shade",
         tl.col ="black", 
         order = "AOE", type = "lower", diag = F)
  
#The correlation matrix indicates the presence of strong autocorrelation between some variables. We should reduce the number of variables in order not to overestimate our model.
```

```{r}
library(PanJen)
formBase <- formula(price_cents~ area + areaPMP + PMP, data=df)
summary(gam(formBase, method="GCV.Cp",data=df))
PanJenArea<-fform(data=df,"area",formBase)
PanJenArea<-fform(data=df,"areaPMP",formBase)
PanJenArea<-fform(data=df,"PMP",formBase)

df$areaPMP2 <- df$areaPMP^2
df$area2 <- df$area^2
```

## Training and test sample division

```{r packages train, message= FALSE, results='hide'}
 library(dplyr)
 library(caret)
```

```{r partition}
  df.sel <- df[,-c(1,3,4)]
  
  set.seed(2021)
  dfPartition <- createDataPartition(y = df.sel$price_cents,
                                    p = 0.7, list = F)
    
  Training <- df.sel[dfPartition,]
  Test <- df.sel[-dfPartition,]
```

## Variables selection

### AIC forward selection

```{r forward variables selection, results='hide'}
Modelzero <- lm(price_cents~1,data=Training)
  summary(Modelzero)
  
  FitAll = lm(price_cents ~ ., data=Training)
  formula(FitAll)
  
  model.forward <- step(Modelzero,direction="forward",scope=formula(FitAll))
```

```{r summary AIC forward}
summary(model.forward)
```

```{r training AIC forward predict and MSE error}
predict.for.tr <- predict(model.forward, newdata = Training)

training.for.mse <- mean((predict.for.tr - Training$price_cents)^2)
paste("Training MSE error:", training.for.mse)
```

```{r test AIC forward predict and MSE error}
predict.for.tst <- predict(model.forward, newdata = Test)

test.for.mse <- mean((predict.for.tst - Test$price_cents)^2)
paste("Test MSE error:", test.for.mse)
```

### AIC backward selection

```{r backward selection, results='hide'}
model.backward <- stepAIC(FitAll, trace=TRUE, direction="backward")
```

```{r summary AIC backward}
summary(model.backward)
```

#### Summary

```{r training AIC backward predict and MSE error}
predict.bck.tr <- predict(model.backward, newdata = Training)

training.bck.mse <- mean((predict.bck.tr - Training$price_cents)^2)
paste("Training MSE error:", training.bck.mse)
```

```{r test AIC backward predict and MSE error}
predict.bck.tst <- predict(model.backward, newdata = Test)

test.bck.mse <- mean((predict.bck.tst - Test$price_cents)^2)
paste("Test MSE error:", test.bck.mse)
```

## Ridge and Lasso regularizations

```{r Training and test matrix}
library(glmnet)

# Convert into a matrix train and test data
train.mat <- model.matrix(price_cents ~ ., data = Training)
test.mat <- model.matrix(price_cents ~ ., data = Test)
```

### Ridge

```{r Ridge - Cross validation}
# Cross validation to obtain the best value of lambda. Error evolution.
cv.ridge <- cv.glmnet(x = train.mat, y = Training$price_cents, alpha = 0, 
                      lambda = NULL, type.measure="mse")

plot(cv.ridge)
paste("Best lambda:", cv.ridge$lambda.min)
paste("Best lambda + y sd:", cv.ridge$lambda.1se)
```

```{r Ridge - Creating and training the model}
# Training the model
mod.ridge.train <- glmnet(x = train.mat, y = Training$price_cents, alpha = 0,
                          lambda = cv.ridge$lambda.1se)

dim(coef(mod.ridge.train))
coef(mod.ridge.train, s = "lambda.1se")
```

```{r Ridge - Model prediction and MSE error}
# Training predictions
pred.ridge <- predict(mod.ridge.train, newx = train.mat)

# Training error (MSE)
tr.ridge.mse <- mean((pred.ridge - Training$price_cents)^2)
paste("Training MSE error:", tr.ridge.mse)
```

```{r Ridge - Test predictions}

#Test predictions: using training model
pred.test.ridge <- predict(mod.ridge.train,newx = test.mat)

test.ridge.mse <- mean((pred.test.ridge - Test$price_cents)^2)
paste("Test MSE error:",test.ridge.mse)
```

### Lasso

```{r Lasso - Cross validation}
cv.lasso <- cv.glmnet(x = train.mat, y = Training$price_cents, alpha = 1, 
                      lambda = NULL, type.measure="mse")

plot(cv.lasso)
paste("Best lambda:", cv.lasso$lambda.min)
paste("Best lambda + y sd:", cv.lasso$lambda.1se)
```

```{r Lasso - Creating and training the model}
# Training the model
mod.lasso.train <- glmnet(x = train.mat, y = Training$price_cents, alpha = 1,
                          lambda = cv.lasso$lambda.1se)

dim(coef(mod.lasso.train))
coef(mod.lasso.train, s = "lambda.1se")
```

```{r Lasso - Model prediction and MSE error}
# Training predictions
pred.lasso <- predict(mod.lasso.train, newx = train.mat)

# Training error (MSE)
tr.lasso.mse <- mean((pred.lasso - Training$price_cents)^2)
paste("Training MSE error:", tr.lasso.mse)
```

```{r Lasso - Test predictions}
#Test predictions: using training model
pred.test.lasso <- predict(mod.lasso.train, newx = test.mat)

test.lasso.mse <- mean((pred.test.lasso - Test$price_cents)^2)
paste("Test MSE error:",test.lasso.mse)
```

### Comparing results

```{r plot comparative}
df_compar <- data.frame(model.comp = c("Forward", "Backward", "Ridge", "Lasso"), 

mse = c(test.for.mse, test.bck.mse, test.ridge.mse, test.lasso.mse))

ggplot(data = df_compar, aes(x = model.comp, y = mse)) + geom_col(width = 0.5) + 
    geom_text(aes(label = round(mse, 2)), vjust = -0.1) + theme_bw() + theme(axis.text.x = element_text(angle = 45, 
    hjust = 1))
```

## **Linear Model**

```{r}
lmodel.Train <- lm (price_cents ~ areaPMP + Locker + Unit, data = Training)

summary(lmodel.Train)
```

```{r}
lmodel.Test <- lm (price_cents ~ areaPMP + Locker + Unit, data = Test)

summary(lmodel.Test)
```
